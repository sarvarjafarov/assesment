DATA_ROLES = [
    {
        'slug': 'head-of-data',
        'title': 'Head of Data',
        'department': 'Data',
        'seniority_level': 'lead',
        'description': 'The Head of Data is responsible for defining and executing the organization-wide data strategy, leading teams across data engineering, analytics, and data science. This role ensures data is treated as a strategic asset, enabling data-driven decision making at every level while maintaining rigorous governance, quality standards, and regulatory compliance.',
        'responsibilities': [
            'Define and execute the enterprise data strategy aligned with business objectives and long-term growth targets',
            'Lead and develop data teams spanning data engineering, analytics, data science, and machine learning functions',
            'Establish data governance frameworks, quality standards, and compliance policies across the organization',
            'Partner with executive leadership to identify high-impact opportunities for data-driven decision making',
            'Own the data infrastructure budget and evaluate build-versus-buy decisions for the data platform',
            'Champion a data-driven culture and ensure data literacy programs reach all business functions',
        ],
        'key_skills': [
            'Strategic planning and organizational leadership for data teams',
            'Data governance, privacy regulations, and regulatory compliance',
            'Data architecture and modern data stack design',
            'Cross-functional stakeholder management and executive influence',
            'Budget management and vendor evaluation for data platforms',
            'Building and scaling high-performing data organizations',
        ],
        'assessment_slugs': ['behavioral'],
        'questions': [
            {
                'question_text': 'How would you approach building a data strategy for an organization that has historically made decisions based on intuition rather than data? What would your first 90 days look like?',
                'category': 'technical',
                'difficulty': 'hard',
                'what_it_tests': 'Ability to design a data transformation roadmap and navigate organizational change from intuition-driven to data-driven culture',
                'sample_answer_outline': 'A strong answer covers an initial assessment phase including stakeholder interviews, data infrastructure audit, and quick-win identification. The candidate should articulate a phased approach: first establishing foundational data quality and access, then building self-service analytics capabilities, and finally enabling advanced analytics and machine learning. They should emphasize change management and executive buy-in as critical success factors alongside the technical roadmap.',
            },
            {
                'question_text': 'Tell me about a time you had to make a difficult trade-off between data quality and speed of delivery when stakeholders needed answers urgently. How did you decide and what was the outcome?',
                'category': 'behavioral',
                'difficulty': 'hard',
                'what_it_tests': 'Pragmatic judgment in balancing data perfectionism with business urgency and ability to communicate trade-offs transparently',
                'sample_answer_outline': 'The candidate should describe a specific scenario with clear business context, explain the competing pressures, and detail the framework they used to evaluate the trade-off. A good answer shows awareness of acceptable risk levels, transparent communication of limitations to stakeholders, and includes a concrete plan for addressing quality gaps after the immediate business need was met.',
            },
            {
                'question_text': 'A business unit leader complains that your data team takes too long to deliver insights and wants to hire their own dedicated analysts outside your organization. How do you handle this situation?',
                'category': 'situational',
                'difficulty': 'hard',
                'what_it_tests': 'Ability to manage stakeholder relationships, organizational design thinking, and balancing centralized versus embedded data team models',
                'sample_answer_outline': 'A strong candidate would first listen to understand the root cause of dissatisfaction, whether it is capacity, prioritization, or communication gaps. They should discuss the trade-offs of centralized versus embedded versus hub-and-spoke data team models, propose a solution that addresses the immediate pain while preserving data consistency, and explain how they would prevent fragmentation of data practices while meaningfully improving responsiveness to business units.',
            },
            {
                'question_text': 'How do you build a data organization that can scale from supporting five internal teams to supporting fifty? What organizational structures, processes, and tooling need to evolve?',
                'category': 'leadership',
                'difficulty': 'hard',
                'what_it_tests': 'Organizational design skills and ability to plan for scale in data team structure, tooling, and self-service capabilities',
                'sample_answer_outline': 'The candidate should discuss evolving from a centralized service model to a more scalable approach such as a data mesh or hub-and-spoke model. They should address self-service tooling investment, data product thinking, documentation and knowledge management, hiring pipelines and career ladder development, and establishing platform teams versus domain-aligned teams. A good answer acknowledges that organizational design must evolve iteratively rather than being designed perfectly upfront.',
            },
            {
                'question_text': 'What is your philosophy on balancing innovation in data and AI with responsible data practices, including privacy, bias mitigation, and ethical use of data?',
                'category': 'culture',
                'difficulty': 'medium',
                'what_it_tests': 'Values around responsible data leadership and ability to set ethical standards while enabling innovation',
                'sample_answer_outline': 'A good answer articulates clear principles around data ethics including transparency in data collection, fairness in algorithmic decision-making, and compliance as a minimum bar rather than a ceiling. The candidate should give examples of governance frameworks they have implemented, how they handle edge cases where business value and ethical concerns conflict, and how they create a culture where team members feel empowered to raise ethical concerns without fear of slowing down projects.',
            },
            {
                'question_text': 'The CEO asks you to quantify the ROI of the entire data organization. How do you measure and communicate the business value that your team delivers?',
                'category': 'problem_solving',
                'difficulty': 'hard',
                'what_it_tests': 'Ability to connect data team output to measurable business outcomes and communicate value to executive stakeholders',
                'sample_answer_outline': 'A strong answer discusses multiple dimensions of data team value: direct revenue impact from data products, cost savings from automation and efficiency, risk reduction from better compliance and data quality, and decision-quality improvement from analytics. The candidate should propose concrete metrics such as time-to-insight reduction, number of data-informed decisions, pipeline revenue influenced by data science models, and operational cost savings, while acknowledging the difficulty of attribution and proposing practical measurement approaches.',
            },
            {
                'question_text': 'Describe your approach to evaluating and selecting a modern data stack. What factors weigh most heavily in your build-versus-buy decisions for data infrastructure?',
                'category': 'technical',
                'difficulty': 'medium',
                'what_it_tests': 'Technical knowledge of the modern data ecosystem and strategic thinking about data infrastructure investment',
                'sample_answer_outline': 'The candidate should demonstrate familiarity with modern data stack components including ingestion, warehousing, transformation, orchestration, and visualization layers. They should discuss evaluation criteria including team skill set alignment, total cost of ownership, scalability trajectory, vendor lock-in risk, integration capabilities, and ongoing maintenance burden. A good answer shows opinionated thinking while acknowledging that the right stack depends on organizational context, team size, and data maturity.',
            },
            {
                'question_text': 'Tell me about a time you successfully championed a major data initiative that required significant investment and faced skepticism from other executives. How did you build the case?',
                'category': 'behavioral',
                'difficulty': 'medium',
                'what_it_tests': 'Executive influence skills and ability to build compelling business cases for data investment',
                'sample_answer_outline': 'The candidate should describe a specific initiative, the source of skepticism, and the approach they took to build a compelling case. A good answer includes quantifying the expected business impact, identifying quick wins to build early momentum and credibility, addressing risk concerns directly, and using storytelling alongside data to persuade. The outcome should demonstrate measurable results that validated the investment thesis.',
            },
            {
                'question_text': 'Your company is acquiring a smaller company with a completely different data stack, data definitions, and data culture. How do you plan the data integration?',
                'category': 'situational',
                'difficulty': 'medium',
                'what_it_tests': 'Strategic planning for data integration during mergers and acquisitions and ability to manage complexity across organizations',
                'sample_answer_outline': 'The candidate should outline a phased approach starting with a thorough assessment of the acquired company data assets, systems, team capabilities, and data quality. They should discuss prioritizing quick integration wins such as unified executive reporting while taking a longer-term approach to platform consolidation. A strong answer addresses people and culture integration alongside technical integration and identifies risks around data quality, conflicting metric definitions, and governance alignment.',
            },
            {
                'question_text': 'How do you ensure that your data leaders and managers are growing in their careers and not stagnating by just managing day-to-day operations?',
                'category': 'leadership',
                'difficulty': 'medium',
                'what_it_tests': 'Commitment to developing data leaders and creating meaningful growth paths within the data organization',
                'sample_answer_outline': 'A good answer discusses creating clear career ladders with both individual contributor and management tracks, providing stretch assignments and cross-functional exposure, investing in external learning and conference participation, and establishing mentorship programs. The candidate should give specific examples of leaders they have developed and describe how they balance relentless operational demands with deliberate investment in people development.',
            },
            {
                'question_text': 'How do you foster collaboration between data scientists who want to use cutting-edge techniques and data engineers who prioritize system reliability and simplicity?',
                'category': 'culture',
                'difficulty': 'easy',
                'what_it_tests': 'Understanding of common tensions within data organizations and ability to build bridges between different data disciplines',
                'sample_answer_outline': 'The candidate should acknowledge this as a common and legitimate tension, then describe approaches such as establishing shared design reviews, creating joint project teams, defining clear interfaces and service-level agreements between teams, and building empathy through rotation programs or shared learning sessions. A good answer emphasizes that both perspectives have genuine merit and the goal is productive collaboration rather than one side winning.',
            },
            {
                'question_text': 'Multiple departments are requesting conflicting definitions for key business metrics like revenue, churn, and active users. How do you resolve this and prevent it from recurring?',
                'category': 'problem_solving',
                'difficulty': 'medium',
                'what_it_tests': 'Ability to establish metric governance and drive organizational alignment on canonical data definitions',
                'sample_answer_outline': 'A strong answer describes a governance process starting with identifying the most critical metrics, convening stakeholders from each department to agree on canonical definitions, documenting definitions in a shared data dictionary or semantic layer, and implementing technical enforcement through certified datasets. The candidate should address how to handle legitimate cases where departments need different cuts of the same metric and how to maintain the definitions over time as the business evolves.',
            },
        ],
    },
    {
        'slug': 'data-scientist',
        'title': 'Data Scientist',
        'department': 'Data',
        'seniority_level': 'senior',
        'description': 'The Data Scientist applies advanced statistical methods, machine learning, and analytical techniques to solve complex business problems and generate actionable insights. This role requires deep technical expertise in modeling and experimentation, strong business acumen to identify high-impact opportunities, and the ability to communicate complex findings to non-technical stakeholders.',
        'responsibilities': [
            'Design and execute end-to-end data science projects from problem framing through model deployment and performance monitoring',
            'Develop and validate predictive models, recommendation systems, and optimization algorithms for business applications',
            'Lead the design and analysis of A/B tests and experiments to measure the causal impact of product and business changes',
            'Collaborate with product, engineering, and business teams to identify opportunities where data science can drive measurable value',
            'Mentor junior data scientists and analysts, reviewing their work and helping them develop technical and analytical skills',
            'Communicate findings and model performance to stakeholders through clear visualizations, presentations, and written documentation',
        ],
        'key_skills': [
            'Machine learning algorithms and statistical modeling techniques',
            'Python with data science libraries such as scikit-learn, pandas, and TensorFlow or PyTorch',
            'Experimental design, A/B testing, and causal inference methods',
            'SQL and experience working with large-scale datasets',
            'Data visualization and storytelling with data',
            'Feature engineering and model evaluation techniques',
        ],
        'assessment_slugs': ['behavioral'],
        'questions': [
            {
                'question_text': 'Walk me through how you would approach building a churn prediction model for a subscription-based product. Cover everything from problem framing to deployment and ongoing monitoring.',
                'category': 'technical',
                'difficulty': 'hard',
                'what_it_tests': 'End-to-end data science project execution skills including problem framing, feature engineering, modeling, and productionization',
                'sample_answer_outline': 'A strong answer starts with defining churn precisely for the business context, choosing the prediction window, and identifying what actions the business would take on predictions. The candidate should discuss data exploration and feature engineering covering usage patterns, engagement signals, support interactions, and billing events. They should compare model approaches such as logistic regression for interpretability versus gradient boosting for performance, explain evaluation metrics including precision-recall trade-offs given class imbalance, and describe deployment considerations including monitoring for model drift and establishing a retraining cadence.',
            },
            {
                'question_text': 'Tell me about a data science project where your initial approach did not work and you had to significantly change direction. What triggered the pivot and what did you learn?',
                'category': 'behavioral',
                'difficulty': 'medium',
                'what_it_tests': 'Resilience, intellectual honesty, and ability to iterate constructively when initial hypotheses or approaches fail',
                'sample_answer_outline': 'The candidate should describe a specific project, explain what went wrong with the initial approach whether it was a modeling issue, data quality problem, or incorrect problem framing, and how they diagnosed the failure. A good answer demonstrates willingness to challenge their own assumptions, a systematic debugging process, and how the pivot led to a better solution. They should convey that changing direction based on evidence was a strength rather than a failure.',
            },
            {
                'question_text': 'A product manager asks you to build a model to predict which customers will upgrade to the premium tier. After thorough analysis, you discover the available features have very weak predictive power. How do you proceed?',
                'category': 'situational',
                'difficulty': 'hard',
                'what_it_tests': 'Ability to manage stakeholder expectations when data does not support the desired outcome and to find alternative paths forward',
                'sample_answer_outline': 'A strong candidate would first validate their finding thoroughly to ensure the weak signal is real, then communicate it honestly to the PM with a clear explanation of why the current features are insufficient. They should propose alternative approaches such as identifying additional data sources that could improve predictive power, suggesting a simpler rule-based approach if the signal is too weak for ML, or reframing the problem to one that the available data can answer. The key is demonstrating that data science value lies in providing honest, actionable guidance, not just building models.',
            },
            {
                'question_text': 'How do you ensure that a junior data scientist you are mentoring is not just building accurate models but is also thinking about the right problems and framing projects effectively from the start?',
                'category': 'leadership',
                'difficulty': 'medium',
                'what_it_tests': 'Mentorship approach and ability to develop problem-framing skills and business acumen in junior team members',
                'sample_answer_outline': 'The candidate should describe specific mentorship techniques such as pairing on problem definition before allowing juniors to dive into modeling, asking guiding questions rather than giving direct answers, reviewing project proposals for problem framing quality before technical work begins, and providing exposure to stakeholder conversations. A good answer emphasizes that technical skills are easier to develop than business judgment and problem framing, and describes how they create safe opportunities for juniors to practice these higher-order skills.',
            },
            {
                'question_text': 'How do you think about fairness and bias when building models that affect real people, such as credit scoring, hiring recommendations, or content ranking systems?',
                'category': 'culture',
                'difficulty': 'medium',
                'what_it_tests': 'Awareness of algorithmic fairness challenges and commitment to responsible model development',
                'sample_answer_outline': 'A good answer demonstrates understanding of different fairness definitions such as demographic parity, equalized odds, and individual fairness, and the inherent tensions between them. The candidate should discuss practical steps including bias auditing during model development, testing for disparate impact across protected groups, involving diverse perspectives in model design, and monitoring deployed models for fairness metrics over time. They should acknowledge that fairness is context-dependent and requires ongoing attention rather than a one-time checkbox.',
            },
            {
                'question_text': 'You built a model that performs well in offline evaluation but shows no measurable improvement in a live A/B test. How do you systematically diagnose this discrepancy?',
                'category': 'problem_solving',
                'difficulty': 'hard',
                'what_it_tests': 'Deep understanding of the gap between offline model performance and real-world impact, and systematic debugging skills',
                'sample_answer_outline': 'A strong answer investigates multiple potential causes: data leakage in the offline evaluation, differences between training data distribution and live traffic, implementation bugs in the serving pipeline, insufficient sample size or test duration, user behavior changes not captured in historical data, and mismatch between the offline metric and the business metric being measured in the A/B test. The candidate should describe a systematic diagnostic process and explain which potential causes they would investigate first and why based on likelihood.',
            },
            {
                'question_text': 'Explain how you would design an A/B test for a pricing change where you need to avoid contamination between treatment and control groups and the change could have long-term effects on customer retention.',
                'category': 'technical',
                'difficulty': 'hard',
                'what_it_tests': 'Experimental design sophistication, particularly for complex business interventions with network effects and long-term outcomes',
                'sample_answer_outline': 'The candidate should discuss randomization unit selection to avoid contamination, such as geographic or cohort-based randomization for pricing experiments. They should address sample size calculation accounting for the expected effect size, measurement of both short-term conversion and long-term retention with appropriate test duration, potential for novelty effects that fade over time, and statistical methods for handling multiple comparison corrections. A good answer also surfaces ethical considerations of price discrimination during testing.',
            },
            {
                'question_text': 'Describe a time when you had to present complex analytical findings to a non-technical executive audience. How did you make the insights accessible and drive concrete action?',
                'category': 'behavioral',
                'difficulty': 'easy',
                'what_it_tests': 'Communication skills and ability to translate technical data science work into business language that drives decisions',
                'sample_answer_outline': 'The candidate should describe a specific presentation, explain the techniques they used to make complex findings accessible such as analogies, progressive disclosure of detail, and focusing on business implications rather than methodology, and share how the audience responded and what actions resulted. A good answer shows genuine skill in distillation rather than simply dumbing things down, and demonstrates understanding that the value of data science is realized only when insights drive action.',
            },
            {
                'question_text': 'Your company wants to build a personalized product experience for each user. The CEO envisions a fully AI-driven system but you have concerns about the data maturity required. How do you navigate this?',
                'category': 'situational',
                'difficulty': 'medium',
                'what_it_tests': 'Ability to manage ambitious stakeholder expectations while being honest about technical prerequisites and data readiness',
                'sample_answer_outline': 'The candidate should validate the vision while being transparent about the maturity journey required to get there. They should propose a crawl-walk-run approach: starting with rule-based segmentation using existing data, progressing to collaborative filtering or simple ML models as data collection improves, and working toward deep personalization as the data foundation matures. A good answer includes specific prerequisites at each stage and frames the roadmap as building toward the CEO vision rather than dismissing it.',
            },
            {
                'question_text': 'How do you decide when a problem genuinely requires a sophisticated machine learning approach versus when a simpler analytical or rule-based solution would be more appropriate and effective?',
                'category': 'leadership',
                'difficulty': 'easy',
                'what_it_tests': 'Pragmatic judgment about solution complexity and ability to choose the right tool for the problem at hand',
                'sample_answer_outline': 'A good answer discusses evaluating factors such as the complexity of the underlying pattern, the volume and quality of available training data, the required accuracy threshold, interpretability requirements from stakeholders, ongoing maintenance burden, and time to initial deployment. The candidate should give concrete examples of when they chose a simpler approach over ML and explain why, demonstrating that they value solving the business problem effectively over applying the most technically sophisticated technique.',
            },
            {
                'question_text': 'What does a healthy data science team culture look like to you? How do you prevent knowledge silos and encourage reproducibility across a team of data scientists?',
                'category': 'culture',
                'difficulty': 'easy',
                'what_it_tests': 'Values around collaboration, knowledge sharing, and scientific rigor within data science teams',
                'sample_answer_outline': 'The candidate should discuss practices such as code reviews for notebooks and model code, standardized project templates and version control, regular knowledge-sharing sessions or journal clubs, shared experiment tracking platforms, and thorough documentation of modeling decisions and assumptions. They should explain why reproducibility matters for both scientific integrity and operational reliability, and how they create a culture where sharing early, imperfect work is encouraged rather than penalized.',
            },
            {
                'question_text': 'A model you deployed six months ago has been performing well, but you just discovered that one of its key features is derived from data that has a subtle but systematic collection bias. What do you do?',
                'category': 'problem_solving',
                'difficulty': 'medium',
                'what_it_tests': 'Ethical responsibility, model governance practices, and ability to handle post-deployment issues systematically',
                'sample_answer_outline': 'A strong answer starts with assessing the severity of the bias and its impact on model predictions and downstream business decisions. The candidate should describe a response plan including immediate stakeholder notification, quantifying the bias effect on past predictions, deciding whether to degrade or disable the affected feature immediately, developing a remediation plan with a timeline, and implementing monitoring to catch similar issues earlier in the future. They should also discuss establishing model governance practices to prevent recurrence such as regular bias audits and feature data quality reviews.',
            },
        ],
    },
    {
        'slug': 'data-engineer',
        'title': 'Data Engineer',
        'department': 'Data',
        'seniority_level': 'mid',
        'description': 'The Data Engineer designs, builds, and maintains the data pipelines and infrastructure that power analytics, reporting, and data science across the organization. This role requires strong software engineering fundamentals combined with expertise in distributed data systems, ensuring that data flows reliably, efficiently, and at scale from source systems to consumption layers.',
        'responsibilities': [
            'Design and build scalable data pipelines for ingesting, transforming, and loading data from diverse source systems',
            'Develop and maintain the data warehouse and data lake architecture to support analytics and reporting needs',
            'Implement data quality checks, monitoring, and alerting to ensure pipeline reliability and data accuracy',
            'Optimize query performance, pipeline efficiency, and cloud infrastructure costs across the data platform',
            'Collaborate with data analysts and data scientists to understand data requirements and model data accordingly',
            'Write and maintain documentation for data schemas, pipeline logic, and operational runbooks',
        ],
        'key_skills': [
            'SQL and Python for data pipeline development',
            'Data warehouse design and dimensional modeling techniques',
            'Data orchestration tools such as Airflow, Dagster, or Prefect',
            'Cloud data platforms including services on AWS, GCP, or Azure',
            'ETL and ELT design patterns for batch and streaming workloads',
            'Version control, testing practices, and CI/CD for data pipelines',
        ],
        'assessment_slugs': ['behavioral'],
        'questions': [
            {
                'question_text': 'Walk me through how you would design a data pipeline architecture for ingesting data from 30 different source systems with varying formats, volumes, and update frequencies into a central data warehouse.',
                'category': 'technical',
                'difficulty': 'hard',
                'what_it_tests': 'Ability to design scalable data ingestion architecture that handles heterogeneous sources and operational complexity',
                'sample_answer_outline': 'A strong answer discusses a layered architecture with a standardized ingestion framework that abstracts source-specific logic, a raw or staging layer that preserves source fidelity, and a transformation layer that conforms data to the warehouse model. The candidate should address batch versus streaming trade-offs for different sources, schema evolution handling, monitoring and alerting strategy for pipeline failures, and how to manage connector maintenance at scale. They should mention specific technologies while showing that architectural principles matter more than individual tool choices.',
            },
            {
                'question_text': 'Tell me about a time a critical data pipeline failed in production and impacted downstream reporting or analytics consumers. How did you diagnose and resolve it?',
                'category': 'behavioral',
                'difficulty': 'hard',
                'what_it_tests': 'Incident response skills and ability to diagnose production data pipeline failures under pressure',
                'sample_answer_outline': 'The candidate should describe a specific incident with clear impact on downstream consumers, explain how they triaged and communicated during the incident, detail the diagnostic process and immediate fix, and describe the root cause analysis afterward. A good answer shows calm systematic debugging under pressure, transparent communication with affected stakeholders, and concrete improvements they implemented to prevent recurrence such as better monitoring, testing, or documentation.',
            },
            {
                'question_text': 'Your data warehouse query costs have tripled over the past quarter and the finance team is pushing for a significant reduction. How do you approach cost optimization without degrading the analytics experience?',
                'category': 'situational',
                'difficulty': 'hard',
                'what_it_tests': 'Cloud cost optimization skills and ability to balance cost reduction with performance and user experience',
                'sample_answer_outline': 'A strong candidate would start with a thorough cost analysis by workload, identifying the largest cost drivers such as expensive ad hoc queries, over-materialized tables, redundant pipelines, or inefficient data models. They should discuss strategies including query optimization guidance for analysts, implementing query governance policies, data lifecycle management with tiered storage, workload scheduling to use off-peak pricing, and potentially rearchitecting expensive pipeline patterns. The answer should include stakeholder communication about any trade-offs and a clear tracking plan to measure cost reduction progress over time.',
            },
            {
                'question_text': 'How do you balance investing time in foundational platform improvements and paying down technical debt versus delivering new data models and pipelines that stakeholders are requesting urgently?',
                'category': 'leadership',
                'difficulty': 'medium',
                'what_it_tests': 'Strategic prioritization and ability to manage technical debt while meeting stakeholder delivery expectations',
                'sample_answer_outline': 'The candidate should describe a framework for allocating engineering time between platform investment and feature delivery, such as reserving a consistent percentage of capacity for infrastructure improvements. They should explain how they communicate the business value of platform investment to non-technical stakeholders using concrete examples of how technical debt creates risk and slows future delivery, and discuss how they identify when accumulated technical debt becomes a critical risk that requires dedicated investment sprints.',
            },
            {
                'question_text': 'What does a healthy engineering culture look like within a data engineering team? How do you maintain high code quality standards while keeping the team motivated and collaborative?',
                'category': 'culture',
                'difficulty': 'medium',
                'what_it_tests': 'Values around team culture, code quality, and maintaining engineering standards in a collaborative way',
                'sample_answer_outline': 'A good answer discusses creating an environment where engineers feel safe to ask questions, admit mistakes during post-mortems, and challenge technical decisions constructively through code reviews. The candidate should describe specific practices such as blameless incident reviews, collaborative code reviews focused on learning rather than gatekeeping, knowledge-sharing sessions, pair programming on complex problems, and celebrating both successful launches and well-handled failures.',
            },
            {
                'question_text': 'Data analysts report that they frequently find inconsistencies between different tables in the warehouse that should contain the same metrics. How do you systematically address this data quality problem?',
                'category': 'problem_solving',
                'difficulty': 'medium',
                'what_it_tests': 'Data quality engineering approach and ability to build systematic quality assurance into data pipelines',
                'sample_answer_outline': 'A strong answer outlines a multi-layered data quality strategy including data contracts at ingestion boundaries, automated data quality checks using tools like Great Expectations or dbt tests at each transformation stage, data lineage tracking to understand the impact of upstream changes, anomaly detection for unexpected metric drift, and a clear ownership model defining who is responsible for quality at each stage. The candidate should discuss how to prioritize which quality issues to fix first based on business impact and downstream consumer visibility.',
            },
            {
                'question_text': 'Explain the trade-offs between a traditional centralized ETL approach with a single data warehouse versus a data mesh architecture with decentralized domain ownership. When would you recommend each?',
                'category': 'technical',
                'difficulty': 'medium',
                'what_it_tests': 'Understanding of modern data architecture paradigms and ability to choose the right approach for the organizational context',
                'sample_answer_outline': 'The candidate should articulate the core principles of each approach, including centralized governance and optimization versus domain autonomy and scalability. They should discuss factors that influence the choice such as organization size and team structure, data complexity, analytical needs, and engineering maturity across the company. A good answer avoids dogmatic advocacy and shows understanding that hybrid approaches are often most practical, and addresses the organizational and cultural prerequisites for a successful data mesh implementation.',
            },
            {
                'question_text': 'Describe a situation where you identified a significant performance bottleneck in a data pipeline and implemented an optimization that made a measurable difference. What was your approach?',
                'category': 'behavioral',
                'difficulty': 'medium',
                'what_it_tests': 'Performance optimization skills and ability to diagnose and resolve efficiency issues in data systems',
                'sample_answer_outline': 'The candidate should describe a specific bottleneck, explain how they identified and profiled it using query plans, monitoring tools, or benchmarking, detail the optimization they implemented such as partitioning strategy changes, query rewrites, caching, or architectural adjustments, and share the measurable improvement. A good answer shows a systematic profiling approach rather than guesswork and demonstrates understanding of the underlying system behavior that caused the bottleneck.',
            },
            {
                'question_text': 'The data science team wants you to build a real-time feature pipeline for a machine learning model, but your current infrastructure is entirely batch-oriented with daily refresh cycles. How do you evaluate and plan this transition?',
                'category': 'situational',
                'difficulty': 'medium',
                'what_it_tests': 'Technical evaluation skills and ability to plan incremental infrastructure transitions collaboratively across teams',
                'sample_answer_outline': 'The candidate should start by understanding the specific use cases and latency requirements driving the request, then evaluate the gap between current batch capabilities and the streaming requirements. They should discuss options ranging from adding a targeted streaming layer alongside the existing batch infrastructure to a more comprehensive architecture change, considering factors like team skill readiness, cost implications, operational complexity, and timeline. A good answer includes a phased rollout plan with clear milestones and risk mitigation.',
            },
            {
                'question_text': 'How do you approach writing data pipeline code that other engineers on your team can easily understand, maintain, and extend after you have moved on to other projects?',
                'category': 'leadership',
                'difficulty': 'easy',
                'what_it_tests': 'Software engineering maturity and commitment to writing maintainable, well-documented data pipeline code',
                'sample_answer_outline': 'A good answer discusses consistent coding standards and project structure, clear naming conventions for pipelines and transformations, comprehensive inline comments explaining business logic rather than just technical implementation, thorough README documentation covering pipeline purpose and dependencies, automated tests that serve as living documentation of expected behavior, and architectural decision records for significant design choices. The candidate should explain why maintainability is especially important in data engineering where pipelines often outlast the engineer who built them.',
            },
            {
                'question_text': 'How do you stay current with the rapidly evolving data engineering tooling landscape without constantly chasing new technologies at the expense of production stability?',
                'category': 'culture',
                'difficulty': 'easy',
                'what_it_tests': 'Approach to continuous learning and pragmatic technology adoption within a data engineering context',
                'sample_answer_outline': 'The candidate should describe practices like dedicated learning time, reading engineering blogs and attending meetups, and conducting small proof-of-concept evaluations for promising new tools. They should explain how they evaluate new technologies critically, considering adoption costs, community maturity, integration effort with existing systems, and whether the tool solves an actual problem the team faces. A good answer balances intellectual curiosity and professional growth with production stability and operational pragmatism.',
            },
            {
                'question_text': 'You inherit a data platform with no documentation, minimal tests, and several pipelines that only one person on the team fully understands. What is your plan to reduce this risk while continuing to deliver new work?',
                'category': 'problem_solving',
                'difficulty': 'hard',
                'what_it_tests': 'Ability to assess and reduce operational risk in legacy data systems while maintaining ongoing delivery commitments',
                'sample_answer_outline': 'A strong answer starts with a risk assessment to identify the most critical and fragile pipelines, then outlines a parallel strategy of documenting existing systems while incrementally adding tests and monitoring to the highest-risk areas. The candidate should discuss conducting knowledge-transfer sessions with the single point of failure, establishing documentation-as-you-go standards for all new and modified work, and building a prioritized backlog of technical debt reduction. They should be realistic about the timeline and the need to interleave improvement work with ongoing feature delivery.',
            },
        ],
    },
    {
        'slug': 'data-analyst',
        'title': 'Data Analyst',
        'department': 'Data',
        'seniority_level': 'junior',
        'description': 'The Data Analyst transforms raw data into actionable insights that drive business decisions across the organization. This role combines strong SQL and analytical skills with growing business domain knowledge to answer critical questions, build dashboards, and identify trends and opportunities that help stakeholders make better, more informed decisions.',
        'responsibilities': [
            'Write and optimize SQL queries to extract, transform, and analyze data from multiple source systems',
            'Build and maintain dashboards and reports that track key business metrics and surface actionable insights',
            'Conduct ad hoc analyses to answer business questions from stakeholders across product, marketing, finance, and operations',
            'Define and document metric definitions, ensuring consistency across teams and reporting tools',
            'Identify trends, anomalies, and opportunities in data and proactively communicate findings to stakeholders',
            'Partner with data engineers to improve data quality and ensure analytical needs are reflected in the data model',
        ],
        'key_skills': [
            'SQL including window functions, CTEs, and query optimization',
            'Business intelligence tools such as Tableau, Looker, or Power BI',
            'Statistical analysis fundamentals and hypothesis testing',
            'Data visualization best practices and storytelling with data',
            'Excel or Google Sheets for modeling and quick analysis',
            'Basic Python or R for data manipulation and exploratory analysis',
        ],
        'assessment_slugs': ['behavioral'],
        'questions': [
            {
                'question_text': 'You are asked to investigate why monthly active users dropped 15 percent last month. Walk me through your analytical approach from the first query to delivering findings to stakeholders.',
                'category': 'technical',
                'difficulty': 'medium',
                'what_it_tests': 'Structured analytical thinking and ability to decompose a metric movement into actionable components',
                'sample_answer_outline': 'A strong answer starts with decomposing the MAU metric into its components: new user acquisition, returning user retention, and churned user reactivation. The candidate should describe segmenting the drop by user cohort, geography, platform, acquisition channel, and product feature usage to isolate which segments drove the decline. They should explain how they would correlate the timing with known events such as product releases, marketing campaign changes, or seasonal patterns, and how they would present findings with recommended next steps rather than just raw observations.',
            },
            {
                'question_text': 'Tell me about a time when your analysis revealed something unexpected that contradicted what stakeholders believed to be true. How did you handle delivering that message?',
                'category': 'behavioral',
                'difficulty': 'medium',
                'what_it_tests': 'Courage to present data-backed findings that challenge assumptions and diplomatic communication skills',
                'sample_answer_outline': 'The candidate should describe a specific situation, explain how they validated the unexpected finding thoroughly before sharing it, and detail how they framed the message to stakeholders. A good answer shows that they presented the data clearly and empathetically, anticipated objections and prepared supporting evidence, and offered constructive next steps. They should demonstrate that they prioritize truth over telling people what they want to hear while being thoughtful about how and when they deliver uncomfortable findings.',
            },
            {
                'question_text': 'The head of sales says their team cannot trust the revenue dashboard because the numbers do not match what they see in the CRM. They want you to fix it immediately. How do you approach this?',
                'category': 'situational',
                'difficulty': 'medium',
                'what_it_tests': 'Ability to diagnose data discrepancy issues while managing stakeholder urgency and rebuilding trust in data',
                'sample_answer_outline': 'The candidate should first acknowledge the stakeholder frustration and commit to investigating promptly. They should describe a systematic reconciliation approach: comparing specific records between the dashboard and CRM, checking for timing differences in data refresh, filter logic discrepancies, metric definition mismatches, and data pipeline freshness issues. A good answer includes communicating findings transparently with a clear root cause explanation, implementing the fix with documentation, and establishing ongoing automated reconciliation checks to prevent future discrepancies.',
            },
            {
                'question_text': 'How do you prioritize when you have multiple stakeholders requesting different analyses simultaneously and each one claims their request is the most urgent and important?',
                'category': 'leadership',
                'difficulty': 'easy',
                'what_it_tests': 'Prioritization skills and ability to manage competing stakeholder expectations in a shared-service analytics role',
                'sample_answer_outline': 'A good answer describes a framework for prioritization based on business impact, time sensitivity, and estimated effort required. The candidate should explain how they communicate realistic timelines transparently, suggest lighter-weight alternatives for lower-priority requests that can still provide value, and escalate to their manager when genuine priority conflicts arise that they cannot resolve. They should also discuss proactive practices like building self-service dashboards and FAQ documentation that reduce ad hoc request volume over time.',
            },
            {
                'question_text': 'What does it mean to you to be a good analytics partner to the business teams you support? How do you build trust with stakeholders who may not be comfortable working with data?',
                'category': 'culture',
                'difficulty': 'easy',
                'what_it_tests': 'Stakeholder orientation and ability to build productive working relationships with non-technical business partners',
                'sample_answer_outline': 'The candidate should discuss being proactive rather than purely reactive, taking time to understand the business context and team goals before diving into data, translating technical findings into plain business language, and following up on how insights were actually used. A good answer emphasizes reliability, responsiveness, and genuine curiosity about the business domain rather than treating analysis as a purely technical exercise disconnected from business outcomes.',
            },
            {
                'question_text': 'You notice that a key metric in a widely-used executive dashboard has been calculated incorrectly for the past three months, but nobody has complained or noticed. What do you do?',
                'category': 'problem_solving',
                'difficulty': 'medium',
                'what_it_tests': 'Integrity in data reporting and ability to handle the uncomfortable discovery of errors in established reporting',
                'sample_answer_outline': 'A strong answer describes immediately quantifying the scope of the error and assessing its potential impact on any decisions that may have been made using the incorrect metric. The candidate should explain how they would fix the calculation, communicate the error transparently to affected stakeholders with a clear explanation and corrected historical data, and implement validation checks such as automated tests or reconciliation logic to prevent similar errors going forward. They should address the uncomfortable reality that errors like this can erode trust and discuss concrete steps to rebuild confidence.',
            },
            {
                'question_text': 'Explain how you would design a dashboard for tracking a new product launch. What metrics would you include, how would you organize them, and what separates an effective dashboard from a collection of charts?',
                'category': 'technical',
                'difficulty': 'medium',
                'what_it_tests': 'Dashboard design skills and understanding of how to organize metrics into a coherent analytical narrative',
                'sample_answer_outline': 'A strong answer discusses starting with the key business questions the dashboard needs to answer, then selecting metrics that form a logical hierarchy from high-level launch outcomes down to diagnostic details. The candidate should describe organizing the dashboard with a summary view and drill-down capability, choosing appropriate chart types for each metric, setting meaningful benchmarks or targets for comparison, and designing for the specific audience who will use it daily. They should articulate what separates a good dashboard from a bad one: clarity of purpose, actionability of the information presented, and appropriate contextual framing.',
            },
            {
                'question_text': 'Describe a time when you went beyond what was originally asked in an analysis and uncovered an insight that had meaningful business impact. What prompted you to dig deeper?',
                'category': 'behavioral',
                'difficulty': 'easy',
                'what_it_tests': 'Intellectual curiosity and proactive analytical mindset that goes beyond answering the literal question asked',
                'sample_answer_outline': 'The candidate should describe a specific analysis where they noticed something unexpected or interesting in the data and chose to investigate further on their own initiative. They should explain what triggered their curiosity, how they pursued the thread while still completing the original request, and what the business impact of the additional discovery was. A good answer demonstrates that great analysts do not just answer the question asked but look for the more important question that should have been asked.',
            },
            {
                'question_text': 'The marketing team has been running campaigns across five channels and wants to understand which channel delivers the best return on investment. However, the attribution data is messy and incomplete. How do you approach this analysis?',
                'category': 'situational',
                'difficulty': 'hard',
                'what_it_tests': 'Ability to produce useful analysis despite imperfect data and to communicate data limitations honestly to stakeholders',
                'sample_answer_outline': 'The candidate should acknowledge the data quality challenges upfront rather than ignoring them or waiting for perfect data. They should describe what they can and cannot reliably measure with the available data, propose practical approaches such as last-touch attribution as a starting point with clear documented caveats, suggest specific improvements to data collection practices for better future analysis, and present findings with explicit confidence levels and limitations. A strong answer prioritizes useful-but-caveated analysis over either waiting indefinitely for perfect data or presenting flawed analysis without disclaimers.',
            },
            {
                'question_text': 'How do you approach learning about a new business domain when you start supporting a team you have never worked with before? What steps do you take in the first few weeks?',
                'category': 'leadership',
                'difficulty': 'easy',
                'what_it_tests': 'Learning agility and deliberate approach to building domain knowledge that makes analytics more relevant and impactful',
                'sample_answer_outline': 'A good answer describes a deliberate onboarding process including meeting with key stakeholders to understand their goals, challenges, and most pressing questions, reviewing existing dashboards and reports to understand current metrics and definitions, sitting in on team meetings to learn the business rhythm and vocabulary, reading relevant industry content, and identifying the most critical recurring decisions the team makes. The candidate should explain how investing in domain knowledge directly improves the quality, relevance, and actionability of their analytical work.',
            },
            {
                'question_text': 'How do you handle a situation where you are asked to present data in a way that you believe is misleading, even if it is technically accurate, to support a decision that has already been made?',
                'category': 'culture',
                'difficulty': 'hard',
                'what_it_tests': 'Integrity in data presentation and willingness to push back constructively on misuse of data for confirmation bias',
                'sample_answer_outline': 'The candidate should clearly state that they would not present analysis they believe is misleading and explain how they would raise their concerns constructively with the requesting stakeholder. They should describe how they would offer alternative ways to present the data that are both accurate and supportive of the broader business context, and how they would handle the situation if pushed to comply despite their objections. A good answer shows understanding of the long-term organizational cost of eroded data trust and their role as a steward of analytical integrity.',
            },
            {
                'question_text': 'A stakeholder asks for a single metric to measure overall customer health, but after exploring the data you realize customer health is genuinely multidimensional and cannot be meaningfully captured in one number. How do you handle this?',
                'category': 'problem_solving',
                'difficulty': 'medium',
                'what_it_tests': 'Ability to balance analytical rigor with stakeholder need for simplicity and to communicate complexity in accessible ways',
                'sample_answer_outline': 'A strong answer describes validating the genuine need for simplicity while transparently educating the stakeholder on the risk of oversimplification. The candidate should propose a composite health score or index that combines multiple dimensions with transparent and documented weighting, provide supplementary drill-down views for the component metrics so stakeholders can understand what drives the score, and document the assumptions and limitations clearly. They should show they can meet the stakeholder where they are while ensuring the resulting metric is not misleading or counterproductive.',
            },
        ],
    },
]
